{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DwzDPiSDyDld"
      },
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9J4ny2ZO_bVq"
      },
      "source": [
        "**IMPORT NECESSARY LIBRARIES**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "7paPZcR6x_OP"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "import tensorflow as tf\n",
        "import string\n",
        "import re\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T9CTLVsI_uWZ"
      },
      "source": [
        "**MOUNT GOOGLE DRIVE**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PkA4-cMQyvfE",
        "outputId": "15d435e0-a1eb-4eb3-d38f-ee3d13ff6fdb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hAhjZ7Zm_yl4"
      },
      "source": [
        "**READ THE DATASET**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "hbdXgc60y6zY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "efd014c0-ca0c-4c95-82c1-b5e34498efa2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Go.\tයන්න.\n",
            "Hi.\tආයුබෝවන්.\n",
            "Run.\tදුවන්න.\n",
            "Who?\tWHO?\n",
            "Wow!\tවාව්!\n",
            "Fire!\tගිනි!\n",
            "Help!\tඋදව්!\n",
            "Jump!\tපනින්න!\n",
            "Jump.\tපනින්න.\n",
            "Stop!\tනවත්වන්න!\n",
            "Wait!\tඉන්න!\n",
            "Go on.\tයන්න.\n",
            "Hello!\tආයුබෝවන්!\n",
            "Hurry!\tඉක්මන් කරන්න!\n",
            "I see.\tමම දකියි.\n",
            "I try.\tමම උත්සාහ කරනවා.\n",
            "I won!\tමම දිනුවා!\n",
            "Oh no!\tඅපොයි නෑ!\n",
            "Relax.\tසන්සුන් වන්න.\n",
            "Shoot!\tවෙඩි තියන්න!\n"
          ]
        }
      ],
      "source": [
        "text_file = \"/content/drive/MyDrive/Colab Notebooks/Mini_project_3/2dataset.txt\"\n",
        "with open(text_file) as f:\n",
        "    lines = f.read().split(\"\\n\")[:-1]\n",
        "i = 0\n",
        "for line in lines:\n",
        "  print(line)\n",
        "  i = i + 1\n",
        "  if(i==20):\n",
        "    break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "iNnVT3Qiy_qP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c93b10ed-4935-4fa9-842a-35a5b0c11c75"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I make it a practice to help my brother with his homework after supper.\tරාත්‍රී ආහාරයෙන් පසු මගේ සහෝදරයාගේ ගෙදර වැඩවලට උදව් කිරීම මම පුරුද්දක් කරගනිමි.\n",
            "Instead of laying off these workers, why don't we just cut their hours?\tමේ කම්කරුවන් දොට්ට දමනවා වෙනුවට අපි ඔවුන්ගේ පැය ගණන කපා නොගන්නේ මන්ද?\n",
            "The thieves pulled open all the drawers of the desk in search of money.\tහොරු සල්ලි හොයන්න මේසයේ ලාච්චු සේරම ඇරියා.\n",
            "Father kept in touch with us by mail and telephone while he was overseas.\tතාත්තා විදේශගතව සිටියදී තැපෑලෙන් සහ දුරකථනයෙන් අපිව සම්බන්ධ කරගත්තා.\n",
            "George Washington was the first president of the Unites States of America.\tජෝර්ජ් වොෂින්ටන් ඇමරිකා එක්සත් ජනපදයේ පළමු ජනාධිපති විය.\n",
            "Mother Teresa used the prize money for her work in India and around the world.\tතෙරේසා මවුතුමිය එම ත්‍යාග මුදල ඉන්දියාවේ සහ ලොව පුරා සිය කටයුතු සඳහා යෙදවූවාය.\n",
            "If you go to that supermarket, you can buy most things you use in your daily life.\tඔබ එම සුපිරි වෙළඳසැලට ගියහොත්, ඔබ එදිනෙදා ජීවිතයේ භාවිතා කරන බොහෝ දේ ඔබට මිලදී ගත හැකිය.\n",
            "The passengers who were injured in the accident were taken to the nearest hospital.\tඅනතුරින් තුවාල ලැබූ මගීන් ළඟම ඇති රෝහල වෙත රැගෙන ගොස් ඇත.\n",
            "Democracy is the worst form of government, except all the others that have been tried.\tප්‍රජාතන්ත්‍රවාදය යනු අත්හදා බලා ඇති අනෙක් සියල්ල හැර නරකම පාලන ක්‍රමයයි.\n",
            "If my boy had not been killed in the traffic accident, he would be a college student now.\tමගේ කොල්ලා රිය අනතුරකින් මිය නොගියේ නම්, ඔහු දැන් විශ්ව විද්‍යාල ශිෂ්‍යයෙකි.\n"
          ]
        }
      ],
      "source": [
        "for x in range(len(lines)-10,len(lines)):\n",
        "  print(lines[x])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T2xEU_wqAC16"
      },
      "source": [
        "**SPLIT THE ENGLISH AND SINHALA TRANSLATION PAIRS**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "X1M4j5V-zEM4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2362525d-9807-4244-e405-eeba22ddc676"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('This machine was manufactured in France.', '[start] මෙම යන්ත්රය ප්රංශයේ නිෂ්පාදනය කරන ලදී. [end]')\n",
            "('I enjoyed swimming in the river.', '[start] මම ගඟේ පිහිනීමට ප්\\u200dරිය කළෙමි. [end]')\n",
            "(\"I couldn't stop myself from longing for her.\", '[start] මට ඇය ගැන ආසාවෙන් ඉන්න එක නවත්වන්න බැරි උනා. [end]')\n"
          ]
        }
      ],
      "source": [
        "text_pairs = []\n",
        "for line in lines:\n",
        "    english, sinhala = line.split(\"\\t\")\n",
        "    sinhala = \"[start] \" + sinhala + \" [end]\"\n",
        "    text_pairs.append((english, sinhala))\n",
        "for i in range(3):\n",
        "  print(random.choice(text_pairs))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S5aVRbpeALO6"
      },
      "source": [
        "**RANDOMIZE THE DATA**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "143xCHTZzWJe"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "random.shuffle(text_pairs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Muw7nYqJAlbd"
      },
      "source": [
        "**SPITING THE DATASET INTO TRAINING, TESTING AND VALIDATION DATA**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "OhE59G2UzZTE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bc5fe47f-79de-4d11-9f4e-8f219d6ebf0c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total sentences: 49201\n",
            "Training set size: 34441\n",
            "Validation set size: 7380\n",
            "Testing set size: 7380\n",
            "Total size of the dataset: 49201\n"
          ]
        }
      ],
      "source": [
        "num_val_samples = int(0.15 * len(text_pairs))\n",
        "num_train_samples = len(text_pairs) - 2 * num_val_samples\n",
        "train_pairs = text_pairs[:num_train_samples]\n",
        "val_pairs = text_pairs[num_train_samples:num_train_samples + num_val_samples]\n",
        "test_pairs = text_pairs[num_train_samples + num_val_samples:]\n",
        "print(\"Total sentences:\",len(text_pairs))\n",
        "print(\"Training set size:\",len(train_pairs))\n",
        "print(\"Validation set size:\",len(val_pairs))\n",
        "print(\"Testing set size:\",len(test_pairs))\n",
        "print(\"Total size of the dataset:\",len(train_pairs)+len(val_pairs)+len(test_pairs))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9tZYKRh_Ayv1"
      },
      "source": [
        "**REMOVING PUNCTUATIONS**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "YT0-IkybzePp",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "864897ba-6ff3-4482-e860-034eb290e5d4"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'[!\"\\\\#\\\\$%\\\\&\\'\\\\(\\\\)\\\\*\\\\+,\\\\-\\\\./:;<=>\\\\?@\\\\\\\\\\\\^_`\\\\{\\\\|\\\\}\\\\~¿]'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "strip_chars = string.punctuation + \"¿\"\n",
        "strip_chars = strip_chars.replace(\"[\", \"\")\n",
        "strip_chars = strip_chars.replace(\"]\", \"\")\n",
        "f\"[{re.escape(strip_chars)}]\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "G1IpzQoCzhML",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "64a6122b-bbf9-43ce-cf11-0e8e120b56a9"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'8'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "f\"{3+5}\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nW-cHLDSA9ge"
      },
      "source": [
        "**VECTORIZATION THE ENGLISH AND SINHALA TEXT PAIRS**\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "-sBCYVZlzkQ2"
      },
      "outputs": [],
      "source": [
        "def custom_standardization(input_string):\n",
        "    lowercase = tf.strings.lower(input_string)\n",
        "    return tf.strings.regex_replace(\n",
        "        lowercase, f\"[{re.escape(strip_chars)}]\", \"\")\n",
        "vocab_size = 15000\n",
        "sequence_length = 20\n",
        "source_vectorization = layers.TextVectorization(\n",
        "    max_tokens=vocab_size,\n",
        "    output_mode=\"int\",\n",
        "    output_sequence_length=sequence_length,\n",
        ")\n",
        "target_vectorization = layers.TextVectorization(\n",
        "    max_tokens=vocab_size,\n",
        "    output_mode=\"int\",\n",
        "    output_sequence_length=sequence_length + 1,\n",
        "    standardize=custom_standardization,\n",
        ")\n",
        "train_english_texts = [pair[0] for pair in train_pairs]\n",
        "train_sinhala_texts = [pair[1] for pair in train_pairs]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "eIk3FECyzt0i"
      },
      "outputs": [],
      "source": [
        "source_vectorization.adapt(train_english_texts)\n",
        "target_vectorization.adapt(train_sinhala_texts)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qgf3qth2BGct"
      },
      "source": [
        "**PREPARATION OF THE DATASET FRO THE TRANSLATION**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "jpsnEAf9z0rs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5b1f4b1c-f3d1-492a-f0db-05fb70e0fe39"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "inputs['english'].shape: (64, 20)\n",
            "inputs['sinhala'].shape: (64, 20)\n",
            "targets.shape: (64, 20)\n",
            "({'english': array([[  50,  124,  989, ...,    0,    0,    0],\n",
            "       [   9,  347,  121, ...,    0,    0,    0],\n",
            "       [  20,   13, 2420, ...,    0,    0,    0],\n",
            "       ...,\n",
            "       [  91,   11,    2, ...,    0,    0,    0],\n",
            "       [  38,    8,    6, ...,    0,    0,    0],\n",
            "       [  44,  782,    4, ...,    0,    0,    0]]), 'sinhala': array([[   2,   12, 1248, ...,    0,    0,    0],\n",
            "       [   2,    7, 3340, ...,    0,    0,    0],\n",
            "       [   2,   10,   54, ...,    0,    0,    0],\n",
            "       ...,\n",
            "       [   2,  290,  484, ...,    0,    0,    0],\n",
            "       [   2,   31, 2351, ...,    0,    0,    0],\n",
            "       [   2, 5607,  589, ...,    0,    0,    0]])}, array([[  12, 1248,  406, ...,    0,    0,    0],\n",
            "       [   7, 3340,  713, ...,    0,    0,    0],\n",
            "       [  10,   54,   45, ...,    0,    0,    0],\n",
            "       ...,\n",
            "       [ 290,  484,  181, ...,    0,    0,    0],\n",
            "       [  31, 2351, 4692, ...,    0,    0,    0],\n",
            "       [5607,  589, 3660, ...,    0,    0,    0]]))\n"
          ]
        }
      ],
      "source": [
        "batch_size = 64\n",
        "def format_dataset(eng, sin):\n",
        "    eng = source_vectorization(eng)\n",
        "    sin = target_vectorization(sin)\n",
        "    return ({\n",
        "        \"english\": eng,\n",
        "        \"sinhala\": sin[:, :-1],\n",
        "    }, sin[:, 1:])\n",
        "def make_dataset(pairs):\n",
        "    eng_texts, sin_texts = zip(*pairs)\n",
        "    eng_texts = list(eng_texts)\n",
        "    sin_texts = list(sin_texts)\n",
        "    dataset = tf.data.Dataset.from_tensor_slices((eng_texts, sin_texts))\n",
        "    dataset = dataset.batch(batch_size)\n",
        "    dataset = dataset.map(format_dataset, num_parallel_calls=4)\n",
        "    return dataset.shuffle(2048).prefetch(16).cache()\n",
        "train_ds = make_dataset(train_pairs)\n",
        "val_ds = make_dataset(val_pairs)\n",
        "for inputs, targets in train_ds.take(1):\n",
        "    print(f\"inputs['english'].shape: {inputs['english'].shape}\")\n",
        "    print(f\"inputs['sinhala'].shape: {inputs['sinhala'].shape}\")\n",
        "    print(f\"targets.shape: {targets.shape}\")\n",
        "inputs['english'].shape: (64, 20)\n",
        "inputs['sinhala'].shape: (64, 20)\n",
        "targets.shape: (64, 20)\n",
        "print(list(train_ds.as_numpy_iterator())[50])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cKTN-IINBYUb"
      },
      "source": [
        "**TRANSFORMER ENCODER IMPLEMENTED AS A SUBCLASSED LAYER**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "0MSpMIu81gb6"
      },
      "outputs": [],
      "source": [
        "class TransformerEncoder(layers.Layer):\n",
        "    def __init__(self, embed_dim, dense_dim, num_heads, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.embed_dim = embed_dim\n",
        "        self.dense_dim = dense_dim\n",
        "        self.num_heads = num_heads\n",
        "        self.attention = layers.MultiHeadAttention(\n",
        "            num_heads=num_heads, key_dim=embed_dim)\n",
        "        self.dense_proj = keras.Sequential(\n",
        "            [layers.Dense(dense_dim, activation=\"relu\"),\n",
        "             layers.Dense(embed_dim),]\n",
        "        )\n",
        "        self.layernorm_1 = layers.LayerNormalization()\n",
        "        self.layernorm_2 = layers.LayerNormalization()\n",
        "    def call(self, inputs, mask=None):\n",
        "        if mask is not None:\n",
        "            mask = mask[:, tf.newaxis, :]\n",
        "        attention_output = self.attention(\n",
        "            inputs, inputs, attention_mask=mask)\n",
        "        proj_input = self.layernorm_1(inputs + attention_output)\n",
        "        proj_output = self.dense_proj(proj_input)\n",
        "        return self.layernorm_2(proj_input + proj_output)\n",
        "    def get_config(self):\n",
        "        config = super().get_config()\n",
        "        config.update({\n",
        "            \"embed_dim\": self.embed_dim,\n",
        "            \"num_heads\": self.num_heads,\n",
        "            \"dense_dim\": self.dense_dim,\n",
        "        })\n",
        "        return config"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ao8PVw1XBnDM"
      },
      "source": [
        "**THE TRANSFORMER DECODER**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "BjI4RLM_1nV_"
      },
      "outputs": [],
      "source": [
        "class TransformerDecoder(layers.Layer):\n",
        "    def __init__(self, embed_dim, dense_dim, num_heads, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.embed_dim = embed_dim\n",
        "        self.dense_dim = dense_dim\n",
        "        self.num_heads = num_heads\n",
        "        self.attention_1 = layers.MultiHeadAttention(\n",
        "            num_heads=num_heads, key_dim=embed_dim)\n",
        "        self.attention_2 = layers.MultiHeadAttention(\n",
        "            num_heads=num_heads, key_dim=embed_dim)\n",
        "        self.dense_proj = keras.Sequential(\n",
        "            [layers.Dense(dense_dim, activation=\"relu\"),\n",
        "             layers.Dense(embed_dim),]\n",
        "        )\n",
        "        self.layernorm_1 = layers.LayerNormalization()\n",
        "        self.layernorm_2 = layers.LayerNormalization()\n",
        "        self.layernorm_3 = layers.LayerNormalization()\n",
        "        self.supports_masking = True\n",
        "    def get_config(self):\n",
        "        config = super().get_config()\n",
        "        config.update({\n",
        "            \"embed_dim\": self.embed_dim,\n",
        "            \"num_heads\": self.num_heads,\n",
        "            \"dense_dim\": self.dense_dim,\n",
        "        })\n",
        "        return config\n",
        "    def get_causal_attention_mask(self, inputs):\n",
        "        input_shape = tf.shape(inputs)\n",
        "        batch_size, sequence_length = input_shape[0], input_shape[1]\n",
        "        i = tf.range(sequence_length)[:, tf.newaxis]\n",
        "        j = tf.range(sequence_length)\n",
        "        mask = tf.cast(i >= j, dtype=\"int32\")\n",
        "        mask = tf.reshape(mask, (1, input_shape[1], input_shape[1]))\n",
        "        mult = tf.concat(\n",
        "            [tf.expand_dims(batch_size, -1),\n",
        "             tf.constant([1, 1], dtype=tf.int32)], axis=0)\n",
        "        return tf.tile(mask, mult)\n",
        "    def call(self, inputs, encoder_outputs, mask=None):\n",
        "        causal_mask = self.get_causal_attention_mask(inputs)\n",
        "        if mask is not None:\n",
        "            padding_mask = tf.cast(\n",
        "                mask[:, tf.newaxis, :], dtype=\"int32\")\n",
        "            padding_mask = tf.minimum(padding_mask, causal_mask)\n",
        "        else:\n",
        "            padding_mask = mask\n",
        "        attention_output_1 = self.attention_1(\n",
        "            query=inputs,\n",
        "            value=inputs,\n",
        "            key=inputs,\n",
        "            attention_mask=causal_mask)\n",
        "        attention_output_1 = self.layernorm_1(inputs + attention_output_1)\n",
        "        attention_output_2 = self.attention_2(\n",
        "            query=attention_output_1,\n",
        "            value=encoder_outputs,\n",
        "            key=encoder_outputs,\n",
        "            attention_mask=padding_mask,\n",
        "        )\n",
        "        attention_output_2 = self.layernorm_2(\n",
        "            attention_output_1 + attention_output_2)\n",
        "        proj_output = self.dense_proj(attention_output_2)\n",
        "        return self.layernorm_3(attention_output_2 + proj_output)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9qXpWKlmBvot"
      },
      "source": [
        "**POSITIONAL ENCODEING**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "Hu6R5TzC1uWa"
      },
      "outputs": [],
      "source": [
        "class PositionalEmbedding(layers.Layer):\n",
        "    def __init__(self, sequence_length, input_dim, output_dim, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.token_embeddings = layers.Embedding(\n",
        "            input_dim=input_dim, output_dim=output_dim)\n",
        "        self.position_embeddings = layers.Embedding(\n",
        "            input_dim=sequence_length, output_dim=output_dim)\n",
        "        self.sequence_length = sequence_length\n",
        "        self.input_dim = input_dim\n",
        "        self.output_dim = output_dim\n",
        "    def call(self, inputs):\n",
        "        length = tf.shape(inputs)[-1]\n",
        "        positions = tf.range(start=0, limit=length, delta=1)\n",
        "        embedded_tokens = self.token_embeddings(inputs)\n",
        "        embedded_positions = self.position_embeddings(positions)\n",
        "        return embedded_tokens + embedded_positions\n",
        "    def compute_mask(self, inputs, mask=None):\n",
        "        return tf.math.not_equal(inputs, 0)\n",
        "    def get_config(self):\n",
        "        config = super(PositionalEmbedding, self).get_config()\n",
        "        config.update({\n",
        "            \"output_dim\": self.output_dim,\n",
        "            \"sequence_length\": self.sequence_length,\n",
        "            \"input_dim\": self.input_dim,\n",
        "        })\n",
        "        return config"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iIoqbFF0B1DU"
      },
      "source": [
        "**END-TO-END TRANSFORMER**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "7xGNmAQU16yQ"
      },
      "outputs": [],
      "source": [
        "embed_dim = 256\n",
        "dense_dim = 2048\n",
        "num_heads = 8\n",
        "encoder_inputs = keras.Input(shape=(None,), dtype=\"int64\", name=\"english\")\n",
        "x = PositionalEmbedding(sequence_length, vocab_size, embed_dim)(encoder_inputs)\n",
        "encoder_outputs = TransformerEncoder(embed_dim, dense_dim, num_heads)(x)\n",
        "decoder_inputs = keras.Input(shape=(None,), dtype=\"int64\", name=\"sinhala\")\n",
        "x = PositionalEmbedding(sequence_length, vocab_size, embed_dim)(decoder_inputs)\n",
        "x = TransformerDecoder(embed_dim, dense_dim, num_heads)(x, encoder_outputs)\n",
        "x = layers.Dropout(0.5)(x)\n",
        "decoder_outputs = layers.Dense(vocab_size, activation=\"softmax\")(x)\n",
        "transformer = keras.Model([encoder_inputs, decoder_inputs], decoder_outputs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "Xbwr2vUG1zVn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0511fb7c-3462-4859-a1c7-21e85a2ad335"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                Output Shape                 Param #   Connected to                  \n",
            "==================================================================================================\n",
            " english (InputLayer)        [(None, None)]               0         []                            \n",
            "                                                                                                  \n",
            " sinhala (InputLayer)        [(None, None)]               0         []                            \n",
            "                                                                                                  \n",
            " positional_embedding (Posi  (None, None, 256)            3845120   ['english[0][0]']             \n",
            " tionalEmbedding)                                                                                 \n",
            "                                                                                                  \n",
            " positional_embedding_1 (Po  (None, None, 256)            3845120   ['sinhala[0][0]']             \n",
            " sitionalEmbedding)                                                                               \n",
            "                                                                                                  \n",
            " transformer_encoder (Trans  (None, None, 256)            3155456   ['positional_embedding[0][0]']\n",
            " formerEncoder)                                                                                   \n",
            "                                                                                                  \n",
            " transformer_decoder (Trans  (None, None, 256)            5259520   ['positional_embedding_1[0][0]\n",
            " formerDecoder)                                                     ',                            \n",
            "                                                                     'transformer_encoder[0][0]'] \n",
            "                                                                                                  \n",
            " dropout (Dropout)           (None, None, 256)            0         ['transformer_decoder[0][0]'] \n",
            "                                                                                                  \n",
            " dense_4 (Dense)             (None, None, 15000)          3855000   ['dropout[0][0]']             \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 19960216 (76.14 MB)\n",
            "Trainable params: 19960216 (76.14 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "transformer.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nfcgAdwWB9Ms"
      },
      "source": [
        "**TRAINING THE TRANSFORMER NEURAL NETWORK**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "p35P05_G2G6O",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9556e249-e3ac-43e1-d901-b8267a6a51a5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/40\n",
            "539/539 [==============================] - 52s 78ms/step - loss: 5.1399 - accuracy: 0.3492 - val_loss: 4.2036 - val_accuracy: 0.4123\n",
            "Epoch 2/40\n",
            "539/539 [==============================] - 35s 65ms/step - loss: 4.0324 - accuracy: 0.4414 - val_loss: 3.6107 - val_accuracy: 0.4776\n",
            "Epoch 3/40\n",
            "539/539 [==============================] - 36s 67ms/step - loss: 3.5051 - accuracy: 0.4929 - val_loss: 3.3434 - val_accuracy: 0.5081\n",
            "Epoch 4/40\n",
            "539/539 [==============================] - 37s 68ms/step - loss: 3.1652 - accuracy: 0.5295 - val_loss: 3.2115 - val_accuracy: 0.5239\n",
            "Epoch 5/40\n",
            "539/539 [==============================] - 36s 68ms/step - loss: 2.9294 - accuracy: 0.5571 - val_loss: 3.1047 - val_accuracy: 0.5422\n",
            "Epoch 6/40\n",
            "539/539 [==============================] - 37s 68ms/step - loss: 2.7464 - accuracy: 0.5801 - val_loss: 3.0772 - val_accuracy: 0.5441\n",
            "Epoch 7/40\n",
            "539/539 [==============================] - 37s 68ms/step - loss: 2.6176 - accuracy: 0.5993 - val_loss: 3.0618 - val_accuracy: 0.5561\n",
            "Epoch 8/40\n",
            "539/539 [==============================] - 37s 68ms/step - loss: 2.5142 - accuracy: 0.6157 - val_loss: 3.0667 - val_accuracy: 0.5595\n",
            "Epoch 9/40\n",
            "539/539 [==============================] - 37s 68ms/step - loss: 2.4419 - accuracy: 0.6281 - val_loss: 3.0432 - val_accuracy: 0.5669\n",
            "Epoch 10/40\n",
            "539/539 [==============================] - 36s 68ms/step - loss: 2.3874 - accuracy: 0.6389 - val_loss: 3.0530 - val_accuracy: 0.5667\n",
            "Epoch 11/40\n",
            "539/539 [==============================] - 38s 70ms/step - loss: 2.3361 - accuracy: 0.6496 - val_loss: 3.0627 - val_accuracy: 0.5681\n",
            "Epoch 12/40\n",
            "539/539 [==============================] - 37s 68ms/step - loss: 2.2981 - accuracy: 0.6582 - val_loss: 3.0622 - val_accuracy: 0.5720\n",
            "Epoch 13/40\n",
            "539/539 [==============================] - 37s 68ms/step - loss: 2.2618 - accuracy: 0.6672 - val_loss: 3.0860 - val_accuracy: 0.5695\n",
            "Epoch 14/40\n",
            "539/539 [==============================] - 37s 68ms/step - loss: 2.2348 - accuracy: 0.6726 - val_loss: 3.0938 - val_accuracy: 0.5702\n",
            "Epoch 15/40\n",
            "539/539 [==============================] - 37s 68ms/step - loss: 2.1974 - accuracy: 0.6800 - val_loss: 3.1438 - val_accuracy: 0.5740\n",
            "Epoch 16/40\n",
            "539/539 [==============================] - 37s 68ms/step - loss: 2.1685 - accuracy: 0.6872 - val_loss: 3.1246 - val_accuracy: 0.5756\n",
            "Epoch 17/40\n",
            "539/539 [==============================] - 38s 70ms/step - loss: 2.1440 - accuracy: 0.6926 - val_loss: 3.1433 - val_accuracy: 0.5768\n",
            "Epoch 18/40\n",
            "539/539 [==============================] - 38s 71ms/step - loss: 2.1219 - accuracy: 0.6975 - val_loss: 3.1778 - val_accuracy: 0.5751\n",
            "Epoch 19/40\n",
            "539/539 [==============================] - 38s 70ms/step - loss: 2.0965 - accuracy: 0.7025 - val_loss: 3.1866 - val_accuracy: 0.5743\n",
            "Epoch 20/40\n",
            "539/539 [==============================] - 38s 70ms/step - loss: 2.0766 - accuracy: 0.7067 - val_loss: 3.2043 - val_accuracy: 0.5767\n",
            "Epoch 21/40\n",
            "539/539 [==============================] - 38s 70ms/step - loss: 2.0598 - accuracy: 0.7109 - val_loss: 3.2599 - val_accuracy: 0.5754\n",
            "Epoch 22/40\n",
            "539/539 [==============================] - 37s 68ms/step - loss: 2.0370 - accuracy: 0.7148 - val_loss: 3.2179 - val_accuracy: 0.5816\n",
            "Epoch 23/40\n",
            "539/539 [==============================] - 36s 68ms/step - loss: 2.0223 - accuracy: 0.7179 - val_loss: 3.2664 - val_accuracy: 0.5757\n",
            "Epoch 24/40\n",
            "539/539 [==============================] - 36s 68ms/step - loss: 1.9988 - accuracy: 0.7223 - val_loss: 3.2954 - val_accuracy: 0.5750\n",
            "Epoch 25/40\n",
            "539/539 [==============================] - 37s 68ms/step - loss: 1.9882 - accuracy: 0.7248 - val_loss: 3.2622 - val_accuracy: 0.5795\n",
            "Epoch 26/40\n",
            "539/539 [==============================] - 37s 68ms/step - loss: 1.9703 - accuracy: 0.7286 - val_loss: 3.3258 - val_accuracy: 0.5769\n",
            "Epoch 27/40\n",
            "539/539 [==============================] - 37s 68ms/step - loss: 1.9547 - accuracy: 0.7315 - val_loss: 3.3462 - val_accuracy: 0.5775\n",
            "Epoch 28/40\n",
            "539/539 [==============================] - 37s 68ms/step - loss: 1.9437 - accuracy: 0.7337 - val_loss: 3.3346 - val_accuracy: 0.5785\n",
            "Epoch 29/40\n",
            "539/539 [==============================] - 37s 68ms/step - loss: 1.9278 - accuracy: 0.7370 - val_loss: 3.3543 - val_accuracy: 0.5792\n",
            "Epoch 30/40\n",
            "539/539 [==============================] - 37s 68ms/step - loss: 1.9139 - accuracy: 0.7397 - val_loss: 3.3529 - val_accuracy: 0.5786\n",
            "Epoch 31/40\n",
            "539/539 [==============================] - 37s 68ms/step - loss: 1.9045 - accuracy: 0.7414 - val_loss: 3.3881 - val_accuracy: 0.5793\n",
            "Epoch 32/40\n",
            "539/539 [==============================] - 37s 68ms/step - loss: 1.8918 - accuracy: 0.7439 - val_loss: 3.3839 - val_accuracy: 0.5785\n",
            "Epoch 33/40\n",
            "539/539 [==============================] - 37s 68ms/step - loss: 1.8899 - accuracy: 0.7443 - val_loss: 3.4537 - val_accuracy: 0.5796\n",
            "Epoch 34/40\n",
            "539/539 [==============================] - 37s 68ms/step - loss: 1.8750 - accuracy: 0.7473 - val_loss: 3.4758 - val_accuracy: 0.5806\n",
            "Epoch 35/40\n",
            "539/539 [==============================] - 37s 69ms/step - loss: 1.8677 - accuracy: 0.7489 - val_loss: 3.4702 - val_accuracy: 0.5775\n",
            "Epoch 36/40\n",
            "539/539 [==============================] - 37s 68ms/step - loss: 1.8561 - accuracy: 0.7511 - val_loss: 3.4993 - val_accuracy: 0.5775\n",
            "Epoch 37/40\n",
            "539/539 [==============================] - 37s 68ms/step - loss: 1.8480 - accuracy: 0.7520 - val_loss: 3.4747 - val_accuracy: 0.5799\n",
            "Epoch 38/40\n",
            "539/539 [==============================] - 37s 68ms/step - loss: 1.8406 - accuracy: 0.7534 - val_loss: 3.5181 - val_accuracy: 0.5806\n",
            "Epoch 39/40\n",
            "539/539 [==============================] - 37s 69ms/step - loss: 1.8280 - accuracy: 0.7560 - val_loss: 3.5150 - val_accuracy: 0.5784\n",
            "Epoch 40/40\n",
            "539/539 [==============================] - 37s 68ms/step - loss: 1.8205 - accuracy: 0.7574 - val_loss: 3.5196 - val_accuracy: 0.5774\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7c72e0307d00>"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ],
      "source": [
        "transformer.compile(\n",
        "    optimizer=\"rmsprop\",\n",
        "    loss=\"sparse_categorical_crossentropy\",\n",
        "    metrics=[\"accuracy\"])\n",
        "transformer.fit(train_ds, epochs=40, validation_data=val_ds)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fZzW_ydnIGlo"
      },
      "source": [
        "**MANUAL TESTING OF THE TRANSLATION MODEL WITH 20 NEW SENTENCES**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "xuE2VEb32sPa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0ddf5fc4-d199-4942-e84d-4068ec1858aa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-\n",
            "There's not enough room in here for both Tom and Mary.\n",
            "[start] ටොම් දෙදෙනාම සිතන්නේ එම දොර ගැන එපා දෙදෙනාම චිත්‍ර සහ දුරකථන අංකය නොවේ [end]\n",
            "-\n",
            "Is it normal?\n",
            "[start] එය සාමාන්‍ය දෙයක් ද [end]\n",
            "-\n",
            "That's the most important thing.\n",
            "[start] එය වැදගත් වන්නේ හොඳම ය [end]\n",
            "-\n",
            "I had to walk home.\n",
            "[start] මට ගෙදර යන්න වුණා [end]\n",
            "-\n",
            "There is no hope that he will come soon.\n",
            "[start] ඔහු ඉක්මනින්ම හෙට ළඟදීම එනවා [end]\n",
            "-\n",
            "I will leave it to your judgement.\n",
            "[start] මම එය ඔබගේ [UNK] පිටත් වෙමි [end]\n",
            "-\n",
            "The engine doesn't work.\n",
            "[start] පසුගිය පසු වැඩ කරන්නේ නැහැ [end]\n",
            "-\n",
            "I got some shampoo in my eyes and it stings.\n",
            "[start] මගේ හිසකෙස් සෝදා ගැනීම මගේ ඇස් වල දුම් පානය කළා [end]\n",
            "-\n",
            "Since he was very drunk, he couldn't drive his car home.\n",
            "[start] ඔහුට මෝටර් රථයක් පැමිණ සිටි පරිදි ඔහුගේ මෝටර් රථය දැඩි ලෙස කෑම වලට නොහැකි විය [end]\n",
            "-\n",
            "Have fun.\n",
            "[start] විහිළු කරන්න [end]\n",
            "-\n",
            "She makes a point of taking a shower before breakfast.\n",
            "[start] ඇය උදේ කෑම කන ගමන් කරයි [end]\n",
            "-\n",
            "We go there often.\n",
            "[start] අපි බොහෝ විට කෑම යනවා [end]\n",
            "-\n",
            "It was hard to persuade him to cancel the trip.\n",
            "[start] ඔහුව දැන ගැනීම සඳහා මම මම යන ලෙස දුෂ්කර විය [end]\n",
            "-\n",
            "Mathematics is difficult for me.\n",
            "[start] ගිටාර් මට දුෂ්කර ය [end]\n",
            "-\n",
            "Are tickets for the concert available here?\n",
            "[start] චිත්‍රපටිය සඳහා ටිකට් යනු සාප්පු සවාරි සැලසුම් තිබේද [end]\n",
            "-\n",
            "Are you going to join the glee club?\n",
            "[start] ඔබ හා [UNK] වීමට යන්නේද [end]\n",
            "-\n",
            "I spoke to Tom yesterday.\n",
            "[start] මම ඊයේ ටොම් එක්ක කතා කළා [end]\n",
            "-\n",
            "Were you the one who wrote this?\n",
            "[start] මේක ලියන්න එකම මාමා [end]\n",
            "-\n",
            "He will pass the next examination.\n",
            "[start] ලබන විභාගය සමත් වනු ඇත [end]\n",
            "-\n",
            "I'll show you around the city.\n",
            "[start] මම ඔබට නගරය හැර දෙන්නම් [end]\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "sin_vocab = target_vectorization.get_vocabulary()\n",
        "sin_index_lookup = dict(zip(range(len(sin_vocab)), sin_vocab))\n",
        "max_decoded_sentence_length = 20\n",
        "\n",
        "def decode_sequence(input_sentence):\n",
        "    tokenized_input_sentence = source_vectorization([input_sentence])\n",
        "    decoded_sentence = \"[start]\"\n",
        "    for i in range(max_decoded_sentence_length):\n",
        "        tokenized_target_sentence = target_vectorization(\n",
        "            [decoded_sentence])[:, :-1]\n",
        "        predictions = transformer(\n",
        "            [tokenized_input_sentence, tokenized_target_sentence])\n",
        "        sampled_token_index = np.argmax(predictions[0, i, :])\n",
        "        sampled_token = sin_index_lookup[sampled_token_index]\n",
        "        decoded_sentence += \" \" + sampled_token\n",
        "        if sampled_token == \"[end]\":\n",
        "            break\n",
        "    return decoded_sentence\n",
        "\n",
        "test_eng_texts = [pair[0] for pair in test_pairs]\n",
        "for _ in range(20):\n",
        "    input_sentence = random.choice(test_eng_texts)\n",
        "    print(\"-\")\n",
        "    print(input_sentence)\n",
        "    print(decode_sequence(input_sentence))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}